# v1.3 ì‚¬ìš© ëª¨ë¸ ì •ë¦¬

## ğŸ“Š ëª¨ë¸ êµ¬ì¡° ê°œìš”

v1.3ì€ **2ë‹¨ê³„ ëª¨ë¸ êµ¬ì¡°**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:
1. **Base Models (v1.2)**: 4-Foldë¡œ ê°ê° í•™ìŠµ
2. **Meta-Classifier**: Foldë³„ ì˜ˆì¸¡ê°’ì„ í†µí•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡

---

## ğŸ”¹ Base Models (v1.2 êµ¬ì¡°)

### 1. SKKUAI AvsHModel (ë¬¸ë‹¨ ê³„ì¸µ êµ¬ì¡°)

**ëª¨ë¸ ì•„í‚¤í…ì²˜**:
```
ë¬¸ì„œ â†’ ë¬¸ë‹¨ ë¶„í•  (ìµœëŒ€ 10ê°œ) 
  â†’ Paragraph Encoder (LLM Backbone) 
  â†’ Transformer Encoder (AvsH) 
  â†’ Classifier (nn.Linear)
```

**Embedding Backbone ì˜µì…˜**:
- `kykim/funnel-kor-base` (ê¸°ë³¸ê°’, 110M íŒŒë¼ë¯¸í„°)
  - **íŠ¹ì§•**: í•œêµ­ì–´ íŠ¹í™”, íš¨ìœ¨ì 
  - **ìš©ë„**: Phase 1, 2, 3ì—ì„œ ì£¼ë¡œ ì‚¬ìš©
  
- `kykim/bert-kor-base` (ëŒ€ì•ˆ)
  - **íŠ¹ì§•**: í•œêµ­ì–´ BERT ê¸°ë°˜
  - **ìš©ë„**: Phase 3, 4ì—ì„œ ì‚¬ìš©

**AvsHModel êµ¬ì„± ìš”ì†Œ**:
- **Paragraph Encoder**: LLM Backbone (Frozen ë˜ëŠ” LoRA)
  - Hidden Size: Backbone ëª¨ë¸ì— ë”°ë¼ ìë™ ì„¤ì •
  - Max Paragraphs: 10ê°œ
  - Max Length: 256-512 í† í°

- **Transformer Encoder**: ë¬¸ë‹¨ ê°„ ê´€ê³„ í•™ìŠµ
  - Layers: 4 (ê¸°ë³¸ê°’)
  - Heads: 8 (ê¸°ë³¸ê°’)
  - Feedforward Dim: 2048 (ê¸°ë³¸ê°’)
  - Dropout: 0.0 (ê¸°ë³¸ê°’)

- **Learnable CLS Token**: ì „ì—­ ë¬¸ë§¥ í‘œí˜„

- **Classifier**: 
  - Output: Binary (0: Human, 1: AI)

**Loss Functions**:
- BCE Loss (ê¸°ë³¸)
- BPR Loss (ì„ íƒì , `--use_bpr_loss`)
- InfoNCE Loss (ì„ íƒì , v1.3ì—ì„œ ì¶”ê°€)

---

### 2. AIGT InfoNCE Loss Models

AIGT í”„ë¡œì íŠ¸ì—ì„œ ì œê³µí•˜ëŠ” **Contrastive Learning** ì§€ì› ëª¨ë¸ë“¤:

#### 2.1 Gemma3-12B (InfoNCE)

**ëª¨ë¸ ì •ë³´**:
- **ëª¨ë¸ëª…**: `google/gemma-3-12b-it`
- **HuggingFace**: https://huggingface.co/google/gemma-3-12b-it
- **íŒŒë¼ë¯¸í„°**: 12B
- **íŠ¹ì§•**: InfoNCE Lossë¡œ Contrastive Learning ì§€ì›

**êµ¬í˜„ íŒŒì¼**:
- `2025-digital-aigt-detection/module/gemma3_seqcls_infonce.py`

**ì£¼ìš” ê¸°ëŠ¥**:
- Standard Loss (BCE/CE/MSE)
- Contrastive Loss (InfoNCE) with temperature
- Adversarial Training (ì„ íƒì )

**Loss ê³„ì‚°**:
```python
Total Loss = Standard Loss + Î»_cl Ã— InfoNCE Loss
```

#### 2.2 Qwen3-14B (InfoNCE)

**ëª¨ë¸ ì •ë³´**:
- **ëª¨ë¸ëª…**: `Qwen/Qwen3-14B`
- **HuggingFace**: https://huggingface.co/Qwen/Qwen3-14B
- **íŒŒë¼ë¯¸í„°**: 14B
- **íŠ¹ì§•**: InfoNCE Lossë¡œ Contrastive Learning ì§€ì›

**êµ¬í˜„ íŒŒì¼**:
- `2025-digital-aigt-detection/module/qwen3_seqcls_infonce.py`

**ì£¼ìš” ê¸°ëŠ¥**:
- Standard Loss (BCE/CE/MSE)
- Contrastive Loss (InfoNCE) with temperature
- Adversarial Training (ì„ íƒì )

#### 2.3 Kanana-8B (ì°¸ê³ ìš©)

**ëª¨ë¸ ì •ë³´**:
- **ëª¨ë¸ëª…**: `kakaocorp/kanana-1.5-8b-instruct-2505`
- **HuggingFace**: https://huggingface.co/kakaocorp/kanana-1.5-8b-instruct-2505
- **íŒŒë¼ë¯¸í„°**: 8B
- **íŠ¹ì§•**: í•œêµ­ì–´-ì˜ì–´ ì´ì¤‘ì–¸ì–´ ëª¨ë¸

**ì°¸ê³ **: AIGT í”„ë¡œì íŠ¸ì—ì„œ ì•™ìƒë¸”ì— ì‚¬ìš©í–ˆìœ¼ë‚˜, v1.3ì—ì„œëŠ” **InfoNCE Loss êµ¬í˜„ì´ ì—†ìŒ**.
- í˜„ì¬ëŠ” Gemma3/Qwen3ë§Œ InfoNCE Loss ì§€ì›

#### 2.4 EXAONE-32B (ì°¸ê³ ìš©)

**ëª¨ë¸ ì •ë³´**:
- **ëª¨ë¸ëª…**: `LGAI-EXAONE/EXAONE-3.5-32B-Instruct`
- **HuggingFace**: https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-32B-Instruct
- **íŒŒë¼ë¯¸í„°**: 32B
- **íŠ¹ì§•**: ëŒ€í˜• í•œêµ­ì–´ ëª¨ë¸

**ì°¸ê³ **: AIGT í”„ë¡œì íŠ¸ì—ì„œ ì•™ìƒë¸”ì— ì‚¬ìš©í–ˆìœ¼ë‚˜, v1.3ì—ì„œëŠ” **InfoNCE Loss êµ¬í˜„ì´ ì—†ìŒ**.
- í˜„ì¬ëŠ” Gemma3/Qwen3ë§Œ InfoNCE Loss ì§€ì›

---

## ğŸ”„ v1.3 Hybrid Model êµ¬ì„±

v1.3ì—ì„œëŠ” **AvsHModel + InfoNCE Loss**ë¥¼ ê²°í•©í•©ë‹ˆë‹¤:

### HybridAvsHModel êµ¬ì¡°

```python
class HybridAvsHModel(AvsHModel):
    """
    AvsHModelì— InfoNCE Lossë¥¼ ì¶”ê°€í•œ í†µí•© ëª¨ë¸
    """
    
    # AvsHModel êµ¬ì¡° ìƒì†
    - Embedding Backbone (Funnel/BERT)
    - Transformer Encoder
    - Classifier
    
    # ì¶”ê°€ ê¸°ëŠ¥
    - Contrastive Learning ì§€ì›
    - InfoNCE Loss ê³„ì‚°
```

**ì„ íƒ ê°€ëŠ¥í•œ êµ¬ì„±**:

| êµ¬ì„± | Embedding Backbone | InfoNCE Loss | ì¶”ì²œ ìš©ë„ |
|------|-------------------|--------------|-----------|
| **Option 1** | `kykim/funnel-kor-base` | âŒ ì—†ìŒ | ë¹ ë¥¸ í•™ìŠµ, ê²½ëŸ‰í™” |
| **Option 2** | `kykim/funnel-kor-base` | âœ… Gemma3/Qwen3 | ê· í˜•ì¡íŒ ì„±ëŠ¥ |
| **Option 3** | `kykim/bert-kor-base` | âœ… Gemma3/Qwen3 | ë†’ì€ ì„±ëŠ¥ |

---

## ğŸ”¹ Meta-Classifier (v1.3 ì¶”ê°€)

4-Fold í•™ìŠµ í›„ **Foldë³„ ì˜ˆì¸¡ê°’(Logits)**ì„ ë©”íƒ€ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©:

### ë©”íƒ€ íŠ¹ì§• ë°ì´í„°ì…‹

```
Shape: [Num_Samples] Ã— 4

Column 0: OOF Logits (Fold 0, Validation ë°ì´í„°)
Column 1: Test Logits (Fold 1, Test ë°ì´í„°)
Column 2: Test Logits (Fold 2, Test ë°ì´í„°)
Column 3: Test Logits (Fold 3, Test ë°ì´í„°)
```

### Meta-Classifier ì˜µì…˜

#### Option 1: MLP (Multi-Layer Perceptron)

**êµ¬ì¡°**:
```
Input (4 features) 
  â†’ Hidden Layer 1 (64 units, ReLU, Dropout=0.2)
  â†’ Hidden Layer 2 (32 units, ReLU, Dropout=0.2)
  â†’ Output (1 unit, Sigmoid)
```

**íŠ¹ì§•**:
- ë¹„ì„ í˜• ê´€ê³„ í•™ìŠµ ê°€ëŠ¥
- ë³µì¡í•œ íŒ¨í„´ ìº¡ì²˜

#### Option 2: Ridge Regression (L2 ì •ê·œí™”)

**êµ¬ì¡°**:
```
Linear Regression with L2 Regularization
```

**íŠ¹ì§•**:
- ë¹ ë¥¸ í•™ìŠµ
- ê³¼ì í•© ë°©ì§€
- í•´ì„ ê°€ëŠ¥

**ì„ íƒ ê¸°ì¤€**:
- **MLP**: ë³µì¡í•œ Fold ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ í•„ìš” ì‹œ
- **Ridge**: ë¹ ë¥¸ í•™ìŠµ ë° ì•ˆì •ì ì¸ ì„±ëŠ¥ í•„ìš” ì‹œ

---

## ğŸ“‹ ì „ì²´ ëª¨ë¸ ìš”ì•½ í…Œì´ë¸”

### Base Models (Foldë³„ í•™ìŠµ)

| ëª¨ë¸ íƒ€ì… | ëª¨ë¸ëª… | íŒŒë¼ë¯¸í„° | ìš©ë„ | Loss |
|-----------|--------|----------|------|------|
| **AvsHModel** | `kykim/funnel-kor-base` | 110M | Embedding Backbone | BCE + BPR |
| **AvsHModel** | `kykim/bert-kor-base` | 110M | Embedding Backbone | BCE + BPR |
| **Gemma3** | `google/gemma-3-12b-it` | 12B | InfoNCE Loss (ì„ íƒ) | BCE + InfoNCE |
| **Qwen3** | `Qwen/Qwen3-14B` | 14B | InfoNCE Loss (ì„ íƒ) | BCE + InfoNCE |

### Meta-Classifier

| ë¶„ë¥˜ê¸° íƒ€ì… | ì…ë ¥ ì°¨ì› | ì¶œë ¥ | ìš©ë„ |
|-------------|-----------|------|------|
| **MLP** | 4 (Fold Logits) | 1 (Binary) | ìµœì¢… ì˜ˆì¸¡ |
| **Ridge** | 4 (Fold Logits) | 1 (Binary) | ìµœì¢… ì˜ˆì¸¡ |

---

## ğŸ¯ v1.3 ê¶Œì¥ êµ¬ì„±

### êµ¬ì„± 1: ê²½ëŸ‰í™” (í•™ìŠµ ì‹œê°„ ìµœì†Œí™”)

```
Base Model:
  - Embedding: kykim/funnel-kor-base
  - InfoNCE Loss: âŒ ì—†ìŒ
  - Loss: BCE + BPR

Meta-Classifier:
  - Type: Ridge Regression
  - ì¥ì : ë¹ ë¥¸ í•™ìŠµ, ì•ˆì •ì  ì„±ëŠ¥
```

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 6-8ì‹œê°„ (4-Fold ë³‘ë ¬)

### êµ¬ì„± 2: ê· í˜• (ì„±ëŠ¥-ì‹œê°„ ê· í˜•)

```
Base Model:
  - Embedding: kykim/funnel-kor-base
  - InfoNCE Loss: âœ… Gemma3/Qwen3
  - Loss: BCE + BPR + InfoNCE (Î»_cl=0.1)

Meta-Classifier:
  - Type: MLP (2 hidden layers)
  - ì¥ì : ë¹„ì„ í˜• ê´€ê³„ í•™ìŠµ
```

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 8-10ì‹œê°„ (4-Fold ë³‘ë ¬)

### êµ¬ì„± 3: ê³ ì„±ëŠ¥ (ìµœëŒ€ ì •í™•ë„)

```
Base Model:
  - Embedding: kykim/bert-kor-base
  - InfoNCE Loss: âœ… Gemma3/Qwen3
  - Loss: BCE + BPR + InfoNCE (Î»_cl=0.2)

Meta-Classifier:
  - Type: MLP (2-3 hidden layers)
  - ì¥ì : ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
```

**ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 10-12ì‹œê°„ (4-Fold ë³‘ë ¬)

---

## ğŸ”§ ëª¨ë¸ ì„¤ì • íŒŒë¼ë¯¸í„°

### AvsHModel íŒŒë¼ë¯¸í„°

```python
# Embedding Backbone
embedding_model = "kykim/funnel-kor-base"  # ë˜ëŠ” "kykim/bert-kor-base"

# Transformer Encoder
num_layers = 4              # Transformer Encoder ë ˆì´ì–´ ìˆ˜
num_heads = 8               # Attention í—¤ë“œ ìˆ˜
dim_feedforward = 2048      # Feedforward ì°¨ì›
dropout = 0.0               # Dropout ë¹„ìœ¨

# í•™ìŠµ íŒŒë¼ë¯¸í„°
max_length = 256            # ë¬¸ë‹¨ë‹¹ ìµœëŒ€ í† í° ìˆ˜
max_paragraphs = 10         # ë¬¸ì„œë‹¹ ìµœëŒ€ ë¬¸ë‹¨ ìˆ˜
learning_rate = 3e-5        # í•™ìŠµë¥ 
num_train_epochs = 10       # ì—í­ ìˆ˜
```

### InfoNCE Loss íŒŒë¼ë¯¸í„°

```python
# InfoNCE Loss í™œì„±í™”
use_infonce_loss = True     # InfoNCE Loss ì‚¬ìš© ì—¬ë¶€
lambda_cl = 0.1             # InfoNCE Loss ê°€ì¤‘ì¹˜
temperature = 0.07          # Temperature íŒŒë¼ë¯¸í„°

# Contrastive Learning
contrastive_labels = labels # ëŒ€ì¡° í•™ìŠµìš© ë ˆì´ë¸”
```

### Meta-Classifier íŒŒë¼ë¯¸í„°

```python
# MLP ì˜µì…˜
meta_model_type = "mlp"
hidden_layers = [64, 32]    # Hidden layer í¬ê¸°
dropout = 0.2               # Dropout ë¹„ìœ¨
activation = "relu"         # í™œì„±í™” í•¨ìˆ˜

# Ridge ì˜µì…˜
meta_model_type = "ridge"
alphas = np.logspace(-4, 2, 25)  # ì •ê·œí™” ê°•ë„ ë²”ìœ„
cv = 5                            # Cross-validation folds
```

---

## ğŸ“Š ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ëª¨ë¸ êµ¬ì„± | GPU ë©”ëª¨ë¦¬ (í•™ìŠµ) | GPU ë©”ëª¨ë¦¬ (ì¶”ë¡ ) | ë°°ì¹˜ í¬ê¸° (ê¶Œì¥) |
|-----------|-------------------|-------------------|------------------|
| **Funnel + AvsH** | 4-6 GB | 2-3 GB | 16-32 |
| **BERT + AvsH** | 5-7 GB | 3-4 GB | 16-32 |
| **Funnel + AvsH + InfoNCE** | 6-8 GB | 3-4 GB | 8-16 |
| **BERT + AvsH + InfoNCE** | 7-10 GB | 4-5 GB | 8-16 |

**ì°¸ê³ **: InfoNCE LossëŠ” Contrastive Learningì„ ìœ„í•´ ì¶”ê°€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

---

## ğŸ” ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### ì§ˆë¬¸ 1: í•™ìŠµ ì‹œê°„ì´ ì œí•œì ì¸ê°€?
- âœ… **Yes** â†’ êµ¬ì„± 1 (ê²½ëŸ‰í™”)
- âŒ **No** â†’ ë‹¤ìŒ ì§ˆë¬¸

### ì§ˆë¬¸ 2: ìµœê³  ì„±ëŠ¥ì´ í•„ìš”í•œê°€?
- âœ… **Yes** â†’ êµ¬ì„± 3 (ê³ ì„±ëŠ¥)
- âŒ **No** â†’ êµ¬ì„± 2 (ê· í˜•)

### ì§ˆë¬¸ 3: GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œê°€?
- âœ… **Yes (â‰¥16GB)** â†’ BERT Backbone + InfoNCE
- âŒ **No (<16GB)** â†’ Funnel Backbone (InfoNCE ì„ íƒì )

### ì§ˆë¬¸ 4: ë³µì¡í•œ íŒ¨í„´ í•™ìŠµì´ í•„ìš”í•œê°€?
- âœ… **Yes** â†’ MLP Meta-Classifier
- âŒ **No** â†’ Ridge Meta-Classifier

---

## ğŸ“ ìš”ì•½

### v1.3ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸

1. **Base Model (v1.2 êµ¬ì¡°)**: 4ê°œ Foldì—ì„œ ê°ê° í•™ìŠµ
   - AvsHModel (ë¬¸ë‹¨ ê³„ì¸µ êµ¬ì¡°)
   - Embedding Backbone: Funnel/BERT
   - Optional: InfoNCE Loss (Gemma3/Qwen3)

2. **Meta-Classifier**: Fold ì˜ˆì¸¡ê°’ í†µí•©
   - MLP ë˜ëŠ” Ridge Regression
   - Input: 4ê°œ Fold Logits
   - Output: ìµœì¢… ì˜ˆì¸¡ (Binary)

### ê¶Œì¥ êµ¬ì„±
- **ê· í˜•í˜•**: Funnel + InfoNCE + MLP
- **ê²½ëŸ‰í˜•**: Funnel + Ridge
- **ê³ ì„±ëŠ¥í˜•**: BERT + InfoNCE + MLP

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **SKKUAI í”„ë¡œì íŠ¸**: `2025_SW_Centered_University_Digital_Competition_SKKUAI/`
- **AIGT í”„ë¡œì íŠ¸**: `2025-digital-aigt-detection/`
- **êµ¬í˜„ ê°€ì´ë“œ**: `v1.3_implementation_guide.md`
- **íŒŒì¼ êµ¬ì¡°**: `v1.3_file_structure.md`

