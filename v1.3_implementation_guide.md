# v1.3 ëª¨ë¸ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì‚´ë ¤ì•¼ í•˜ëŠ” ì½”ë“œ íŒŒì¼](#ì‚´ë ¤ì•¼-í•˜ëŠ”-ì½”ë“œ-íŒŒì¼)
3. [v1.3 ì¬êµ¬ì„± êµ¬ì¡°](#v1.3-ì¬êµ¬ì„±-êµ¬ì¡°)
4. [êµ¬í˜„ ë‹¨ê³„ë³„ ê°€ì´ë“œ](#êµ¬í˜„-ë‹¨ê³„ë³„-ê°€ì´ë“œ)

---

## ê°œìš”

v1.3ì€ v1.2 ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ **4-Fold ì•™ìƒë¸” ê¸°ë²•**ì„ í†µí•´ ì •í™•ë„ì™€ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

### v1.2 ëª¨ë¸ êµ¬ì¡° (ì¬ì‚¬ìš©)
- **SKKUAI ëª¨ë¸ (AvsHModel)**: ë¬¸ë‹¨ ë‹¨ìœ„ ê³„ì¸µì  í•™ìŠµ
- **AIGT ëª¨ë¸ (InfoNCE Loss)**: Contrastive Learning ì§€ì›
- **í˜¼í•© ì „ëµ**: ë‘ ëª¨ë¸ì˜ ì¥ì  ê²°í•©

### v1.3 ì¶”ê°€ êµ¬ì¡°
- **4-Fold Cross-Validation**: ê° Foldë³„ ë…ë¦½ í•™ìŠµ
- **Meta-Learning**: Fold ì˜ˆì¸¡ê°’ì„ ë©”íƒ€ íŠ¹ì§•ìœ¼ë¡œ í™œìš©
- **Meta-Classifier**: MLP/Ridgeë¥¼ í†µí•œ ìµœì¢… ì˜ˆì¸¡

---

## ì‚´ë ¤ì•¼ í•˜ëŠ” ì½”ë“œ íŒŒì¼

### 1. SKKUAI í”„ë¡œì íŠ¸ í•µì‹¬ íŒŒì¼

#### ğŸ“ ëª¨ë¸ ê´€ë ¨
- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/models/AvsHModel.py`
  - **ìš©ë„**: v1.2ì˜ í•µì‹¬ ëª¨ë¸ (ë¬¸ë‹¨ ê³„ì¸µ êµ¬ì¡°)
  - **ë³€ê²½ í•„ìš”**: ì—†ìŒ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)

- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/models/get_model.py`
  - **ìš©ë„**: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
  - **ë³€ê²½ í•„ìš”**: AIGT ëª¨ë¸ (Gemma3/Qwen3) ì§€ì› ì¶”ê°€

#### ğŸ“ ë°ì´í„° ê´€ë ¨
- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/datasets_ours/text_dataset.py`
  - **ìš©ë„**: ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ë¬¸ì„œ â†’ ë¬¸ë‹¨ ë¶„í• )
  - **ë³€ê²½ í•„ìš”**: ì—†ìŒ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)

- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/datasets_ours/text_collator.py`
  - **ìš©ë„**: ë°°ì¹˜ ë°ì´í„° ì „ì²˜ë¦¬ (ë¬¸ë‹¨ í† í¬ë‚˜ì´ì§•)
  - **ë³€ê²½ í•„ìš”**: ì—†ìŒ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)

- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/datasets_ours/get_dataset.py`
  - **ìš©ë„**: K-Fold ë°ì´í„° ë¶„í• 
  - **ë³€ê²½ í•„ìš”**: ì—†ìŒ (ì´ë¯¸ 4-Fold ì§€ì›)

#### ğŸ“ í•™ìŠµ ê´€ë ¨
- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/train.py`
  - **ìš©ë„**: í•™ìŠµ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
  - **ë³€ê²½ í•„ìš”**: Foldë³„ í•™ìŠµ ë° Logit ì €ì¥ ë¡œì§ ì¶”ê°€

- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/text_trainer.py`
  - **ìš©ë„**: ì»¤ìŠ¤í…€ Trainer (BCE + BPR Loss)
  - **ë³€ê²½ í•„ìš”**: InfoNCE Loss ì§€ì› ì¶”ê°€ (AIGT í†µí•©)

- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/utils/compute_metrics.py`
  - **ìš©ë„**: í‰ê°€ ë©”íŠ¸ë¦­ (ROC-AUC ë“±)
  - **ë³€ê²½ í•„ìš”**: Logit ì €ì¥ ë¡œì§ ì¶”ê°€

- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/arguments.py`
  - **ìš©ë„**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
  - **ë³€ê²½ í•„ìš”**: AIGT ëª¨ë¸ ê´€ë ¨ ì¸ì ì¶”ê°€

- âœ… `2025_SW_Centered_University_Digital_Competition_SKKUAI/additional_loss.py`
  - **ìš©ë„**: BPR Loss êµ¬í˜„
  - **ë³€ê²½ í•„ìš”**: ì—†ìŒ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)

### 2. AIGT í”„ë¡œì íŠ¸ í•µì‹¬ íŒŒì¼

#### ğŸ“ ëª¨ë¸ ê´€ë ¨
- âœ… `2025-digital-aigt-detection/module/gemma3_seqcls_infonce.py`
  - **ìš©ë„**: Gemma3 ê¸°ë°˜ InfoNCE Loss ëª¨ë¸
  - **ë³€ê²½ í•„ìš”**: SKKUAI Trainerì™€ í†µí•©

- âœ… `2025-digital-aigt-detection/module/qwen3_seqcls_infonce.py`
  - **ìš©ë„**: Qwen3 ê¸°ë°˜ InfoNCE Loss ëª¨ë¸
  - **ë³€ê²½ í•„ìš”**: SKKUAI Trainerì™€ í†µí•©

### 3. ìƒˆë¡œ ìƒì„±í•´ì•¼ í•˜ëŠ” íŒŒì¼

#### ğŸ“ v1.3 ì „ìš© íŒŒì¼
- ğŸ†• `v1.3/fold_trainer.py`
  - **ìš©ë„**: Foldë³„ í•™ìŠµ ë° Logit ìƒì„± ê´€ë¦¬
  - **ê¸°ëŠ¥**: 
    - 4-Fold ìˆœì°¨/ë³‘ë ¬ í•™ìŠµ
    - OOF Logits (Fold 1) ì €ì¥
    - Test Logits (Fold 2,3,4) ì €ì¥

- ğŸ†• `v1.3/meta_dataset.py`
  - **ìš©ë„**: Meta-Features ë°ì´í„°ì…‹ ìƒì„±
  - **ê¸°ëŠ¥**:
    - Foldë³„ Logits ìˆ˜ì§‘
    - `[Num_Samples] x 4` í˜•íƒœ ë©”íƒ€ íŠ¹ì§• ìƒì„±

- ğŸ†• `v1.3/meta_classifier.py`
  - **ìš©ë„**: Meta-Classifier (MLP/Ridge)
  - **ê¸°ëŠ¥**:
    - ë©”íƒ€ íŠ¹ì§• ì…ë ¥ â†’ ìµœì¢… ì˜ˆì¸¡
    - Cross-Validation ì§€ì›

- ğŸ†• `v1.3/train_fold.py`
  - **ìš©ë„**: Foldë³„ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
  - **ê¸°ëŠ¥**:
    - ë‹¨ì¼ Fold í•™ìŠµ ì‹¤í–‰
    - Logit ì €ì¥ ìë™í™”

- ğŸ†• `v1.3/inference_fold.py`
  - **ìš©ë„**: Foldë³„ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
  - **ê¸°ëŠ¥**:
    - í•™ìŠµëœ ëª¨ë¸ë¡œ Test Logits ìƒì„±

- ğŸ†• `v1.3/meta_train.py`
  - **ìš©ë„**: Meta-Classifier í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
  - **ê¸°ëŠ¥**:
    - ë©”íƒ€ íŠ¹ì§• ë°ì´í„°ì…‹ ë¡œë”©
    - Meta-Classifier í•™ìŠµ ë° í‰ê°€

- ğŸ†• `v1.3/meta_inference.py`
  - **ìš©ë„**: ìµœì¢… ì˜ˆì¸¡ ìƒì„±
  - **ê¸°ëŠ¥**:
    - Test Logits â†’ Meta-Classifier â†’ ìµœì¢… ì˜ˆì¸¡

---

## v1.3 ì¬êµ¬ì„± êµ¬ì¡°

### ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
v1.3/
â”œâ”€â”€ models/                          # ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ avsh_model.py                # SKKUAI AvsHModel (ì¬ì‚¬ìš©)
â”‚   â”œâ”€â”€ gemma3_infonce.py            # AIGT Gemma3 (ì¬ì‚¬ìš©)
â”‚   â”œâ”€â”€ qwen3_infonce.py             # AIGT Qwen3 (ì¬ì‚¬ìš©)
â”‚   â””â”€â”€ hybrid_model.py              # v1.2: AvsH + InfoNCE í†µí•©
â”‚
â”œâ”€â”€ datasets/                        # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_dataset.py              # SKKUAI (ì¬ì‚¬ìš©)
â”‚   â”œâ”€â”€ text_collator.py             # SKKUAI (ì¬ì‚¬ìš©)
â”‚   â”œâ”€â”€ get_dataset.py               # SKKUAI (ì¬ì‚¬ìš©, 4-Fold ì§€ì›)
â”‚   â””â”€â”€ meta_dataset.py              # ğŸ†• Meta-Features ìƒì„±
â”‚
â”œâ”€â”€ trainers/                        # í•™ìŠµ ë¡œì§
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hybrid_trainer.py            # v1.2: AvsH + InfoNCE Trainer
â”‚   â””â”€â”€ fold_trainer.py              # ğŸ†• Foldë³„ í•™ìŠµ ê´€ë¦¬
â”‚
â”œâ”€â”€ meta/                            # Meta-Learning
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ meta_classifier.py           # ğŸ†• MLP/Ridge Meta-Classifier
â”‚   â”œâ”€â”€ meta_train.py                # ğŸ†• Meta-Classifier í•™ìŠµ
â”‚   â””â”€â”€ meta_inference.py            # ğŸ†• ìµœì¢… ì˜ˆì¸¡
â”‚
â”œâ”€â”€ scripts/                         # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train_fold.sh                # ğŸ†• Foldë³„ í•™ìŠµ (4ê°œ Fold ë³‘ë ¬)
â”‚   â”œâ”€â”€ inference_fold.sh            # ğŸ†• Foldë³„ ì¶”ë¡ 
â”‚   â”œâ”€â”€ meta_train.sh                # ğŸ†• Meta-Classifier í•™ìŠµ
â”‚   â””â”€â”€ meta_inference.sh            # ğŸ†• ìµœì¢… ì˜ˆì¸¡
â”‚
â”œâ”€â”€ utils/                           # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ compute_metrics.py           # SKKUAI (ì¬ì‚¬ìš©)
â”‚   â”œâ”€â”€ arguments.py                 # SKKUAI (ìˆ˜ì • í•„ìš”)
â”‚   â”œâ”€â”€ losses.py                    # BPR + InfoNCE Loss í†µí•©
â”‚   â””â”€â”€ logit_collector.py           # ğŸ†• Fold Logits ìˆ˜ì§‘
â”‚
â”œâ”€â”€ configs/                         # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ fold_config.yaml             # ğŸ†• Fold í•™ìŠµ ì„¤ì •
â”‚   â””â”€â”€ meta_config.yaml             # ğŸ†• Meta-Classifier ì„¤ì •
â”‚
â””â”€â”€ outputs/                         # ì¶œë ¥ ë””ë ‰í† ë¦¬
    â”œâ”€â”€ fold_logits/                 # Foldë³„ Logits
    â”‚   â”œâ”€â”€ oof/                     # OOF Logits (Fold 1)
    â”‚   â””â”€â”€ test/                    # Test Logits (Fold 2,3,4)
    â”œâ”€â”€ meta_features/               # Meta-Features Dataset
    â”‚   â”œâ”€â”€ meta_train.csv
    â”‚   â””â”€â”€ meta_test.csv
    â””â”€â”€ final_predictions/           # ìµœì¢… ì˜ˆì¸¡
        â””â”€â”€ submission.csv
```

---

## êµ¬í˜„ ë‹¨ê³„ë³„ ê°€ì´ë“œ

### ğŸ”¹ Step 1: v1.2 ëª¨ë¸ í†µí•© (Hybrid Model)

**ëª©í‘œ**: SKKUAI AvsHModel + AIGT InfoNCE Loss ê²°í•©

**ì‘ì—…**:
1. `v1.3/models/hybrid_model.py` ìƒì„±
   - AvsHModel êµ¬ì¡° ìœ ì§€
   - Forwardì—ì„œ InfoNCE Loss ê³„ì‚° ì§€ì›
   - Contrastive Learning ì˜µì…˜ ì¶”ê°€

2. `v1.3/trainers/hybrid_trainer.py` ìƒì„±
   - TextTrainer ìƒì†
   - BCE Loss + BPR Loss + InfoNCE Loss í†µí•©
   - Loss ê°€ì¤‘ì¹˜ ì¡°ì ˆ ê°€ëŠ¥

**í•µì‹¬ ì½”ë“œ ìŠ¤ë‹ˆí«**:
```python
# hybrid_model.py
class HybridAvsHModel(AvsHModel):
    def forward(self, input_ids, attention_mask=None, 
                contrastive_labels=None, lambda_cl=1.0, **kwargs):
        # ê¸°ì¡´ AvsHModel forward
        logits, paragraph_logits = super().forward(...)
        
        # InfoNCE Loss ê³„ì‚° (ì„ íƒì )
        if contrastive_labels is not None:
            features = self.get_cls_embeddings(...)
            cl_loss = compute_infonce_loss(features, contrastive_labels)
            return logits, paragraph_logits, cl_loss
        
        return logits, paragraph_logits
```

---

### ğŸ”¹ Step 2: Foldë³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸

**ëª©í‘œ**: 4-Foldë¡œ v1.2 ëª¨ë¸ í•™ìŠµ ë° Logit ìƒì„±

**ì‘ì—…**:
1. `v1.3/trainers/fold_trainer.py` ìƒì„±
   - Foldë³„ í•™ìŠµ ë£¨í”„
   - OOF/Test Logit ì €ì¥ ìë™í™”

2. `v1.3/scripts/train_fold.sh` ìƒì„±
   - 4ê°œ Fold ë³‘ë ¬ í•™ìŠµ (ë˜ëŠ” ìˆœì°¨)
   - ê° Foldë³„ ë…ë¦½ ì‹¤í–‰

3. `v1.3/utils/logit_collector.py` ìƒì„±
   - Foldë³„ Logits ìˆ˜ì§‘
   - ë©”íƒ€ íŠ¹ì§• ë°ì´í„°ì…‹ ìƒì„±

**í•µì‹¬ ë¡œì§**:
```python
# fold_trainer.py
class FoldTrainer:
    def train_fold(self, fold_idx, args):
        # 1. ë°ì´í„° ë¡œë“œ (K-Fold ë¶„í• )
        train_ds, val_ds = get_dataset(args, tokenizer, fold_idx)
        
        # 2. ëª¨ë¸ í•™ìŠµ
        trainer = HybridTrainer(...)
        trainer.train()
        
        # 3. OOF Logits ìƒì„± (Fold 1ë§Œ)
        if fold_idx == 0:
            oof_logits = trainer.predict(val_ds)
            save_oof_logits(oof_logits, fold_idx)
        
        # 4. Test Logits ìƒì„± (Fold 2,3,4)
        if fold_idx > 0:
            test_logits = trainer.predict(test_ds)
            save_test_logits(test_logits, fold_idx)
```

**Fold ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸**:
```bash
# train_fold.sh
for FOLD in 0 1 2 3; do
    python train_fold.py \
        --fold_idx ${FOLD} \
        --k_fold 4 \
        --model_type hybrid \
        --output_dir "./outputs/fold_${FOLD}" &
done
wait  # ëª¨ë“  Fold ì™„ë£Œ ëŒ€ê¸°
```

---

### ğŸ”¹ Step 3: Meta-Features ë°ì´í„°ì…‹ ìƒì„±

**ëª©í‘œ**: Foldë³„ Logitsë¥¼ ë©”íƒ€ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜

**ì‘ì—…**:
1. `v1.3/datasets/meta_dataset.py` ìƒì„±
   - Foldë³„ Logits ë¡œë”©
   - `[Num_Samples] x 4` í˜•íƒœ ë³€í™˜

2. `v1.3/scripts/create_meta_features.sh` ìƒì„±
   - Logits ìˆ˜ì§‘ ë° CSV ë³€í™˜

**í•µì‹¬ ë¡œì§**:
```python
# meta_dataset.py
class MetaDataset:
    def __init__(self, logit_dir):
        # OOF Logits (Fold 1)
        oof_logits = np.load(f"{logit_dir}/oof/fold0_logits.npy")
        oof_labels = np.load(f"{logit_dir}/oof/fold0_labels.npy")
        
        # Test Logits (Fold 2,3,4)
        test_logits = []
        for fold in [1, 2, 3]:
            logits = np.load(f"{logit_dir}/test/fold{fold}_logits.npy")
            test_logits.append(logits)
        
        # ë©”íƒ€ íŠ¹ì§• ìƒì„±: [Num_Samples] x 4
        self.meta_features = np.stack([oof_logits] + test_logits, axis=1)
        self.labels = oof_labels
```

---

### ğŸ”¹ Step 4: Meta-Classifier êµ¬í˜„

**ëª©í‘œ**: MLP/Ridgeë¡œ ìµœì¢… ì˜ˆì¸¡

**ì‘ì—…**:
1. `v1.3/meta/meta_classifier.py` ìƒì„±
   - MLP ì˜µì…˜
   - Ridge Regression ì˜µì…˜
   - Cross-Validation ì§€ì›

2. `v1.3/meta/meta_train.py` ìƒì„±
   - Meta-Classifier í•™ìŠµ
   - Hyperparameter Tuning

**í•µì‹¬ ë¡œì§**:
```python
# meta_classifier.py
class MetaClassifier:
    def __init__(self, model_type='mlp'):
        if model_type == 'mlp':
            self.model = MLPClassifier(
                hidden_layers=[64, 32],
                activation='relu',
                dropout=0.2
            )
        elif model_type == 'ridge':
            self.model = RidgeClassifierCV(
                alphas=np.logspace(-4, 2, 25),
                cv=5,
                scoring='roc_auc'
            )
    
    def fit(self, X_meta, y):
        """X_meta: [Num_Samples] x 4"""
        self.model.fit(X_meta, y)
    
    def predict_proba(self, X_meta):
        return self.model.predict_proba(X_meta)
```

---

### ğŸ”¹ Step 5: ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©

**ëª©í‘œ**: End-to-End ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

**ì‘ì—…**:
1. `v1.3/run_pipeline.py` ìƒì„±
   - ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìë™í™”
   - ë‹¨ê³„ë³„ ê²€ì¦

2. `v1.3/scripts/run_all.sh` ìƒì„±
   - ëª¨ë“  ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰

**íŒŒì´í”„ë¼ì¸**:
```python
# run_pipeline.py
def main():
    # Step 1: Foldë³„ í•™ìŠµ (5-7ì‹œê°„ Ã— 4)
    print("Step 1: Training 4 Folds...")
    for fold in range(4):
        train_fold(fold_idx=fold)
    
    # Step 2: Meta-Features ìƒì„±
    print("Step 2: Creating Meta-Features...")
    create_meta_features()
    
    # Step 3: Meta-Classifier í•™ìŠµ
    print("Step 3: Training Meta-Classifier...")
    train_meta_classifier()
    
    # Step 4: ìµœì¢… ì˜ˆì¸¡
    print("Step 4: Generating Final Predictions...")
    generate_final_predictions()
    
    print("âœ… Pipeline Complete!")
```

---

## ğŸ“Š ë°ì´í„° íë¦„ë„

```
[Main Dataset]
      â”‚
      â–¼
[Paragraph Decomposition]  (ìµœëŒ€ 10ë¬¸ë‹¨)
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4-Fold Training (v1.2 ëª¨ë¸)        â”‚
â”‚  - Fold 0: Train â†’ OOF Logits       â”‚
â”‚  - Fold 1,2,3: Train â†’ Test Logits  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
[Meta-Features Dataset]
  Shape: [Num_Samples] Ã— 4
  - Column 0: OOF Logits (Fold 0)
  - Column 1-3: Test Logits (Fold 1,2,3)
      â”‚
      â–¼
[Meta-Classifier Training]
  - MLP ë˜ëŠ” Ridge
  - Input: [Num_Samples] Ã— 4
  - Output: ìµœì¢… ì˜ˆì¸¡ (0: Human, 1: AI)
      â”‚
      â–¼
[Final Prediction & Submission]
```

---

## ğŸ”§ ì£¼ìš” ìˆ˜ì • ì‚¬í•­ ìš”ì•½

### 1. Model í†µí•©
- âœ… AvsHModel + InfoNCE Loss í†µí•©
- âœ… Contrastive Learning ì˜µì…˜ ì¶”ê°€

### 2. Trainer í™•ì¥
- âœ… HybridTrainer: BCE + BPR + InfoNCE Loss
- âœ… FoldTrainer: Foldë³„ í•™ìŠµ ê´€ë¦¬

### 3. ë°ì´í„° ì²˜ë¦¬
- âœ… K-Fold ì§€ì› (ì´ë¯¸ êµ¬í˜„ë¨)
- âœ… Logit ìˆ˜ì§‘ ë° ë©”íƒ€ íŠ¹ì§• ìƒì„±

### 4. Meta-Learning
- âœ… Meta-Classifier êµ¬í˜„
- âœ… ìµœì¢… ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ íŒŒì¼ ì¬ì‚¬ìš©
- [x] AvsHModel.py
- [x] text_dataset.py
- [x] text_collator.py
- [x] get_dataset.py (K-Fold ì§€ì›)
- [x] gemma3_seqcls_infonce.py
- [x] qwen3_seqcls_infonce.py
- [x] compute_metrics.py

### ìƒˆë¡œ ìƒì„±í•  íŒŒì¼
- [ ] hybrid_model.py
- [ ] hybrid_trainer.py
- [ ] fold_trainer.py
- [ ] meta_dataset.py
- [ ] meta_classifier.py
- [ ] meta_train.py
- [ ] meta_inference.py
- [ ] train_fold.py
- [ ] logit_collector.py
- [ ] run_pipeline.py

### ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼
- [ ] train_fold.sh
- [ ] inference_fold.sh
- [ ] meta_train.sh
- [ ] meta_inference.sh
- [ ] run_all.sh

---

## â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„

| ë‹¨ê³„ | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ |
|------|------|----------|
| Step 1 | v1.2 ëª¨ë¸ í†µí•© | 2-3ì‹œê°„ |
| Step 2 | Foldë³„ í•™ìŠµ (4ê°œ) | 5-7ì‹œê°„ Ã— 4 = 20-28ì‹œê°„ |
| Step 3 | Meta-Features ìƒì„± | 10ë¶„ |
| Step 4 | Meta-Classifier í•™ìŠµ | 30ë¶„-1ì‹œê°„ |
| Step 5 | ìµœì¢… ì˜ˆì¸¡ | 10ë¶„ |
| **ì´ê³„** | | **24-32ì‹œê°„** |

**ë³‘ë ¬ ì²˜ë¦¬ ì‹œ**: Foldë³„ í•™ìŠµì„ 4ê°œ GPUì—ì„œ ë³‘ë ¬ ì‹¤í–‰ â†’ **8-10ì‹œê°„**

---

## ğŸ¯ ìµœì¢… ëª©í‘œ

1. **ì •í™•ë„ í–¥ìƒ**: 4-Fold ì•™ìƒë¸”ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
2. **í•™ìŠµ ì‹œê°„ ìµœì†Œí™”**: SKKUAI + AIGT íš¨ìœ¨ì  í†µí•©
3. **ì¼ë°˜í™” ì„±ëŠ¥**: Meta-Learningìœ¼ë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ

---

## ğŸ“š ì°¸ê³ ì‚¬í•­

- **GPU ë¦¬ì†ŒìŠ¤**: 4ê°œ Fold ë³‘ë ¬ í•™ìŠµ ì‹œ ìµœì†Œ 4Ã—A100 í•„ìš”
- **ë©”ëª¨ë¦¬**: ê° Foldë‹¹ ì•½ 4-8GB GPU ë©”ëª¨ë¦¬ í•„ìš”
- **ì €ì¥ê³µê°„**: Logits ì €ì¥ ì‹œ ì•½ 500MB-1GB í•„ìš”

